{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<ul>\n",
    "    <li>Nearest neighbor approaches are called <b>lazy learners</b> because each time an instance is classified, we have to comb through the entire training set.</li>\n",
    "    <li>Bayesian models are called <b>eager learners</b>. When given a training set, it builds the model, and then to classify instances, uses this internal model. Eager learners <i>tend to classify instances faster than lazy learners</i></li>\n",
    "</ul> \n",
    "\n",
    "The ability to make probablistic predictions along with the fact that they are eager learners are two big advantages of Bayesian methods. \n",
    "\n",
    "<b>P(h|d)</b> means the probability of \"h\" happening given some condition \"D\" (posterior probability)\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)\\cap P(B)}{P(B)} $$ \n",
    "\n",
    "<ul> \n",
    "    <li><b>P(h):</b> prior probability</li> \n",
    "    <li><b>P(h|d):</b> posterior probability</li>\n",
    "    <li><b>P(D) and P(D|h):</b> will be needed for Bayes</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$ P(h|D) = \\frac{P(D|h)P(h)}{P(D)} $$ \n",
    "\n",
    "You can <b>think of classes as hypotheses</b>- once we do this, we pick the class (hypothesis) with the highest probability. \n",
    "\n",
    "<b>Maximum a posteriori hypothesis:</b> pick the hypothesis with the highest probability. \n",
    "\n",
    "The core idea behind Bayesian probability here is illustrated in the example below: \n",
    "<ul>\n",
    "    <li>There is a form of cancer that affects 0.3% of people</li> \n",
    "    <li>There is a test that identifies whether you have the cancer or not with 98% accuracy.</li> \n",
    "    <li><b>IF A RANDOM PERSON TESTS AS POSITIVE, THEY ARE STILL NOT THAT LIKELY TO HAVE THE CANCER!</b> In fact, it is far more likely they do not. I cannot stress this enough.</li> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Take each P(D|h), multiply them together, then <b>multiply by P(h)</b>. Whichever hypothesis yields the higher Bayesian probability can be classified as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i500\n",
      "republican\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "class BayesClassifier: \n",
    "    def __init__(self, bucketPrefix, testBucketNumber, dataFormat):\n",
    "        counts = {}\n",
    "        classes = {}\n",
    "        total = 0\n",
    "        \n",
    "        #same stuff as before with the buckets\n",
    "        self.format = dataFormat.strip().split('\\t')\n",
    "        self.prior = {}\n",
    "        self.conditional = {}\n",
    "        \n",
    "        numericValues = {}\n",
    "        totals = {}\n",
    "        \n",
    "        self.ssd = {}\n",
    "        self.means = {}\n",
    "        \n",
    "        hasNums = False \n",
    "        \n",
    "        for i in range(1,11): \n",
    "            if i != testBucketNumber:\n",
    "                filename = \"%s-%02i\" % (bucketPrefix, i)\n",
    "                f = open(filename)\n",
    "                lines = f.readlines() \n",
    "                f.close() \n",
    "                for line in lines:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    ignore = []\n",
    "                    vector = []\n",
    "                    nums = []\n",
    "                    for i in range(len(fields)):\n",
    "                        if self.format[i] == 'num':\n",
    "                            hasNums = True\n",
    "                            nums.append(float(fields[i]))\n",
    "                        elif self.format[i] == 'attr':\n",
    "                            vector.append(fields[i])\n",
    "                        elif self.format[i] == 'comment':\n",
    "                            ignore.append(fields[i])\n",
    "                        elif self.format[i] == 'class':\n",
    "                            category = fields[i]\n",
    "                    total += 1 \n",
    "                    classes.setdefault(category, 0)\n",
    "                    counts.setdefault(category,{})\n",
    "                    classes[category] += 1 \n",
    "                    totals.setdefault(category, {})\n",
    "                    numericValues.setdefault(category, {})\n",
    "                    col = 0 \n",
    "                    for columnValue in vector: \n",
    "                        #count 'em up \n",
    "                        col += 1 \n",
    "                        counts[category].setdefault(col ,{})\n",
    "                        counts[category][col].setdefault(columnValue, 0)\n",
    "                        counts[category][col][columnValue]+= 1 \n",
    "                    \n",
    "                    col = 0 \n",
    "                    for columnValue in nums: \n",
    "                        col += 1 \n",
    "                        totals[category].setdefault(col, 0)\n",
    "                        totals[category][col] += columnValue \n",
    "                        numericValues[category].setdefault(col, [])\n",
    "                        numericValues[category][col].append(columnValue)\n",
    "                        \n",
    "        #calculate probability for each class\n",
    "        for (category, count) in classes.iteritems():\n",
    "            self.prior[category] = count / total\n",
    "        \n",
    "        for (category, columns) in counts.iteritems():\n",
    "            self.conditional.setdefault(category, {})\n",
    "            for (col, valueCounts) in columns.iteritems():\n",
    "                self.conditional[category].setdefault(col, {})\n",
    "                for (attrValue, count) in valueCounts.items():\n",
    "                    #calculate the probability of CONDITION / CLASS \n",
    "                    self.conditional[category][col][attrValue] = count / classes[category]\n",
    "\n",
    "        #EXTRA stuff we need to do if the classifier has to deal with numbers \n",
    "        #namely, we have to generate statistics for each column (mean, ssd)\n",
    "        if hasNums: \n",
    "            for(category, columns) in totals.iteritems():\n",
    "                self.means.setdefault(category,{})\n",
    "                for(col, cTotal) in columns.iteritems():\n",
    "                    self.means[category][col]= cTotal / classes[category]\n",
    "                    \n",
    "            for(category, columns) in numericValues.iteritems():\n",
    "                self.ssd.setdefault(category, {})\n",
    "                for (col, values) in columns.iteritems(): \n",
    "                    sumsd = 0 \n",
    "                    tmean = self.means[category][col]\n",
    "                    for val in values: \n",
    "                        sumsd += (val - tmean) ** 2 \n",
    "                    columns[col] = 0\n",
    "                    self.ssd[category][col] = (sumsd / (classes[category] - 1)) ** (0.5)\n",
    "    \n",
    "    def classify(self, itemVec, numVector): \n",
    "        results = []\n",
    "        for (category, prior) in self.prior.items():\n",
    "            prob = prior \n",
    "            col = 1 \n",
    "            if len(itemVec) > 0: \n",
    "                #how do the categorical elements factor into the probability? \n",
    "                for attrValue in itemVec:\n",
    "                    if not attrValue in self.conditional[category][col]:\n",
    "                        prob = 0 \n",
    "                    else: \n",
    "                        prob = prob * self.conditional[category][col][attrValue]\n",
    "                    col += 1 \n",
    "                    \n",
    "            #how the continuous attributes factor into the probability \n",
    "            if len(numVector) > 0: \n",
    "                col = 1 \n",
    "                for x in numVector:\n",
    "                    #calculate probability that value exists at that point on the normal\n",
    "                    #distrobution based on sample-generated heuristics\n",
    "                    mean = self.means[category][col]\n",
    "                    ssd = self.ssd[category][col]\n",
    "                    e = math.pow(math.e, -(x-mean) ** 2 / (2 * ssd ** 2))\n",
    "                    prob = prob * ((1.0 / math.sqrt(2 * math.pi))*e)\n",
    "                    col += 1 \n",
    "            results.append((prob, category))\n",
    "        #return hypothesis with maximum probability \n",
    "        return(max(results)[1])\n",
    "\n",
    "c = BayesClassifier(\"iHealth/i\", 10, \"attr\\tattr\\tattr\\tattr\\tclass\")\n",
    "print(c.classify(['health', 'moderate', 'moderate', 'yes'], []))\n",
    "\n",
    "#WOOOHHH! it works. NOW LETS LOAD SOME POLITICAL DATA! \n",
    "\n",
    "d = BayesClassifier(\"house-votes/hv\", 10, \"class\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\")\n",
    "print(d.classify(['yes','no','yes','yes','no','yes','yes','yes','no','yes','no','yes','no','yes','no','no'], []))\n",
    "\n",
    "e = BayesClassifier(\"pimaSmall/pimaSmall\", 1, \"num\\tnum\\tnum\\tnum\\tnum\\tnum\\tnum\\tnum\\tclass\")\n",
    "print(e.classify([], [3,78,50,32,88,31,0.248,26]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an event that <b>hasn't occurred yet</b> (like \"Democrat votes no for Healthcare\" or something), and it is in the attribute set of an item we are trying to classify, it will <b>shoot the probability down to zero no matter what!</b> In many cases, this is illogical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities in Naive Bayes are <b>estimates</b> of true probabilities. How can we fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our formula now for each probability is shown below: \n",
    "\n",
    "$$ P(x|y) = \\frac{n_c}{n} $$\n",
    "\n",
    "This has some drawbacks, namely if something hasn't occured before, the entire value goes to zero. \n",
    "\n",
    "$$ P(x|y) = \\frac{n_c + mp}{n + m} $$\n",
    "\n",
    "<b>m</b> is a constant called the equivalent sample size. Many methods to calculate it, but for starters, we can use the numbers of classes. <b>p</b> is the prior probability of an event. \n",
    "\n",
    "<b>m</b> does not have to remain constant! If there are 3 options (easy, medium, hard), m would be 3 and p would be 1/3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been working with discrete categories instead of continuous. This makes sense- 100 is closer than 105, but a Saxophonist cannot be said to be closer to a Pianist. <b>How to use Bayesian methods for topics that fit less nicely into discrete categories?</b> \n",
    "\n",
    "### Method 1: Making Categories\n",
    "\n",
    "Split numbers into into intervals, with each interval as a distinct class. \n",
    "\n",
    "### Method 2: Gaussian Distrobutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population standard deviation and sample standard deviation \n",
    "<b>Sample standard deviation</b> is the same as that for <b>population standard deviation</b> except with <b>n-1</b> in the denominator. \n",
    "$$ P(x_i|y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{i,j}}e^{\\frac{-(x_i - \\mu_{i,j})^2}{2\\sigma^2_{i,j}}} $$ \n",
    "\n",
    "This formula gives the probability that x is in class y given its value, and assuming the values are scattered over a standard distrobution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Naive Bayes vs. kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pros</b> for Naive Bayes: \n",
    "<ul>\n",
    "    <li><b>Simple</b> to implement!</li>\n",
    "    <li>Needs <b>less</b> training data than many other methods</li> \n",
    "    <li>Performs quick operations, and gives results fast.</li> \n",
    "</ul> \n",
    "\n",
    "Main <b>con</b> for Naive Bayes: cannot learn interactions among features. (e.g. I like food with cheese and cream but not both)\n",
    "\n",
    "<b>Pros</b> for kNN: \n",
    "<ul>\n",
    "    <li>Also <b>simple</b> to implement</li> \n",
    "    <li>Does not assume data has a particular structure</li>\n",
    "    <li><b>Large amount of memory</b> needed to store training set</li>\n",
    "</ul> \n",
    "\n",
    "<b>k Nearest Neighbors</b> makes sense when we have huge amounts of training data. Used in image classification, recommender systems, etc.\n",
    "\n",
    "<b>NAIVE BAYES WORKS ON INDEPENDENT PROBABILITIES, BUT MOST REAL-WORLD ATTRIBUTES AREN'T INDEPENDENT!</b> This is what makes it \"Naive\"- we are assuming independence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
