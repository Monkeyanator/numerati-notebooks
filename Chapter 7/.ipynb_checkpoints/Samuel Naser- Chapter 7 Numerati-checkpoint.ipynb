{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying Unstructured Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "In all cases we have studied thus far, datasets have been <b>easily represented in a table</b> \n",
    "\n",
    "This kind of data is called <b><u>structured data</u></b>. Unstructured data includes emails, tweets, blog posts, and newspaper articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## An automatic system for determining positive and negative texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Perhaps you are a politician, and would like to gague sentiment on a recent speech. Perhaps you are a corporation, and would like to know what people have to say about your product. \n",
    "\n",
    "<b>Possible idea:</b> we could have a list of <i>like</i> words and a list of <i>dislike</i> words, each providing evidence that a person likes or dislikes the product. \n",
    "\n",
    "Now, we <i>could</i> use raw counts, but instead, lets give our old friend <b>Naive Bayes</b> a ring!!!!!\n",
    "\n",
    "$$ h_M = argmax P(D|h)P(h) $$\n",
    "\n",
    "Since we are now involved in unstructured text, our dataset is called <b>the training corpus</b>. \n",
    "\n",
    "Each entry in the training corpus is called <b><u>a document</u></b>. Each document, in our case, is labeled as \"favorable\" or \"unfavorable\"\n",
    "\n",
    "If there are 1000 documents with 500 favorable and 500 unfavorable, then <b>P(favorable) = 0.5</b> and <b>P(unfavorable) = 0.5</b> \n",
    "\n",
    "When we start with <i>labeled training data</i>, the task is called <b><u>supervised learning</u></b>.\n",
    "\n",
    "Learning from <i>unlabeled training data</i> is called <b><u>unsupervised learning</u></b>. \n",
    "\n",
    "So here's what we're gonna do: we're gonna treat the document as a bag of words, and compute probabilities like <b>what is the probability this document contains the word \"sunshine\" and is a \"favorable\" document?</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training phase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we determine the <b><u>vocabulary</u></b>, or the list of unique words, in all of the documents (like + dislike). <b>|Vocabulary|</b> denotes the number of terms in the vocabulary.\n",
    "\n",
    "$$ P(w_i|h_i) $$ \n",
    "\n",
    "The above form denotes probability that a word occurs in a document given a hypothesis (like vs. dislike). This can be calculated in the following steps: \n",
    "\n",
    "<ol>\n",
    "    <li><b>Combine</b> documents tagged with hypothesis into <b>one text file</b></li>\n",
    "    <li><b>Count</b> word occurences in the aggregated file. If there are 300 occurences of \"the\", let's count it 300 times. Call this <b>n</b></li>\n",
    "    <li>For <b>each word in vocabulary</b> count how many times word occurred in text.</li>\n",
    "    <li>For each word in vocabulary, <b>compute</b> the following expression:</li>\n",
    "    $$P(w_k|h_i) = \\frac{n_k + 1}{n + |Vocabulary|}$$\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes Classification Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the standard formula from above, \n",
    "\n",
    "$$ h_M = argmax P(D|h)P(h) $$\n",
    "\n",
    "Let's use this sentence as an example: <b>\"I knew the movie would be trash before the title sequence even rolled\"</b>\n",
    "\n",
    "Each word gets assigned a probability for <b>each hypothesis</b>. For example: \"trash\" might have <b>P(word|like) = 0.00009</b> and <b>P(word|dislike) = 0.02 </b>, indicating you are more likely to find the word in a negative review. \n",
    "\n",
    "Compute the product <b> P(like) * P(I|like) * P(knew|like) * ... </b> and then compute the product <b> P(dislike) * P(I|dislike) * P(knew|dislike) * ... </b>. \n",
    "\n",
    "The hypothesis with the higher probability will be our prediction. \n",
    "\n",
    "<b><u>THIS RESULTS IN SOME SERIOUSLY SMALL NUMBERS!!! YIKES!</u></b>\n",
    "\n",
    "Instead of multiplying the fractions, we can just <b>add the logs</b> instead. (Default base of Python log is \"e\")\n",
    "\n",
    "In general, <b>log compresses the scale of a number</b>. For example: 0.0000001 * 0.000005 = 0.0000000000005. The log equivalent of these numbers are -16.11809 + -9.90348 = -26.02157"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Newsgroup Corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>First, we can remove the most common words</b>, since most words in sentences don't carry much meaning past syntax. \n",
    "\n",
    "We call these useless words <b><u>stop words</u></b>, and there are many stop word datasets online. \n",
    "\n",
    "### Stop words can be useful: DO NOT BLINDLY REMOVE THEM! \n",
    "\n",
    "Think first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform count operation...\n",
      "->alt.atheism\n",
      "->comp.graphics\n",
      "->comp.os.ms-windows.misc\n",
      "->comp.sys.ibm.pc.hardware\n",
      "->comp.sys.mac.hardware\n",
      "->comp.windows.x\n",
      "->misc.forsale\n",
      "->rec.autos\n",
      "->rec.motorcycles\n",
      "->rec.sport.baseball\n",
      "->rec.sport.hockey\n",
      "->sci.crypt\n",
      "->sci.electronics\n",
      "->sci.med\n",
      "->sci.space\n",
      "->soc.religion.christian\n",
      "->talk.politics.guns\n",
      "->talk.politics.mideast\n",
      "->talk.politics.misc\n",
      "->talk.religion.misc\n",
      "Computing probabilities! :D\n",
      "->alt.atheism\n",
      "->comp.graphics\n",
      "->comp.os.ms-windows.misc\n",
      "->comp.sys.ibm.pc.hardware\n",
      "->comp.sys.mac.hardware\n",
      "->comp.windows.x\n",
      "->misc.forsale\n",
      "->rec.autos\n",
      "->rec.motorcycles\n",
      "->rec.sport.baseball\n",
      "->rec.sport.hockey\n",
      "->sci.crypt\n",
      "->sci.electronics\n",
      "->sci.med\n",
      "->sci.space\n",
      "->soc.religion.christian\n",
      "->talk.politics.guns\n",
      "->talk.politics.mideast\n",
      "->talk.politics.misc\n",
      "->talk.religion.misc\n",
      "Training completed.\n",
      "\n",
      "\n",
      "PROBABILITY OF WORD GOD IN MOTORCYCLES:  0.000172430596685\n",
      "PROBABILITY OF WORD GOD IN SOC RELIGION:  0.00659474107349\n",
      "\n",
      "ACCURACY: 0.795539 , 7532 test instances\n"
     ]
    }
   ],
   "source": [
    "import os, math, codecs \n",
    "\n",
    "class BayesText: \n",
    "    \n",
    "    def __init__(self, training_dir, stopword_list): \n",
    "        self.vocab = {}\n",
    "        self.prob = {}\n",
    "        self.totals = {}\n",
    "        self.stopwords = {}\n",
    "\n",
    "        swFile = open(stopword_list) \n",
    "        for line in swFile.readlines():\n",
    "            self.stopwords[line.strip()] = 1 \n",
    "        swFile.close() \n",
    "        \n",
    "        #get list of categories from names of folders in train dir\n",
    "        categories = os.listdir(training_dir)\n",
    "        self.categories = [filename for filename in categories if os.path.isdir(training_dir + '/' + filename)]\n",
    "        print \"Perform count operation...\"\n",
    "        for category in self.categories: \n",
    "            print '->' + category \n",
    "            (self.prob[category], self.totals[category]) = self.train(training_dir, category)\n",
    "        toBeDeleted = []\n",
    "        for word in self.vocab.keys():\n",
    "            if self.vocab[word] < 3: \n",
    "                toBeDeleted.append(word)\n",
    "        for word in toBeDeleted: \n",
    "            del self.vocab[word]\n",
    "        vocabLength = len(self.vocab)\n",
    "        print \"Computing probabilities! :D\"\n",
    "        for category in self.categories: \n",
    "            print '->' + category\n",
    "            denominator = self.totals[category] + vocabLength\n",
    "            for word in self.vocab: \n",
    "                if word in self.prob[category]:\n",
    "                    count = self.prob[category][word]\n",
    "                else: \n",
    "                    count = 1 \n",
    "                self.prob[category][word] = float(count + 1) / denominator\n",
    "            \n",
    "        print \"Training completed.\\n\"\n",
    "    \n",
    "    def train(self, training_dir, category): \n",
    "        current_dir = training_dir + '/' + category \n",
    "        files = os.listdir(current_dir)\n",
    "        counts = {}\n",
    "        total = 0\n",
    "        for file in files: \n",
    "            f = codecs.open(current_dir + '/' + file, 'r', 'iso8859-1')\n",
    "            for line in f: \n",
    "                tokens = line.split()\n",
    "                for token in tokens: \n",
    "                    token = token.strip('\\'\".,?:-')\n",
    "                    token = token.lower() \n",
    "                    if token != '' and not token in self.stopwords: \n",
    "                        self.vocab.setdefault(token, 0)\n",
    "                        self.vocab[token] += 1 \n",
    "                        counts.setdefault(token, 0)\n",
    "                        counts[token] += 1 \n",
    "                        total += 1 \n",
    "            f.close() \n",
    "        return (counts,total)\n",
    "    \n",
    "    def classify(self, train_file): \n",
    "        results = {}\n",
    "        for category in self.categories: \n",
    "            results[category] = 0 \n",
    "        f = codecs.open(train_file, 'r', 'iso8859-1')\n",
    "        for line in f: \n",
    "            tokens = line.split() \n",
    "            for token in tokens: \n",
    "                token = token.strip('\\'\".,?:-').lower() \n",
    "                if token in self.vocab: \n",
    "                    for category in self.categories: \n",
    "                        if self.prob[category][token] == 0: \n",
    "                            print \"%s %s\" % (category, token)\n",
    "                        results[category] += math.log(self.prob[category][token])\n",
    "        f.close() \n",
    "        results = list(results.items())\n",
    "        results.sort(key= lambda t: t[1], reverse = True)\n",
    "        \n",
    "        return results[0][0]\n",
    "                        \n",
    "    def testCat(self, directory, category): \n",
    "        files = os.listdir(directory)\n",
    "        total = 0 \n",
    "        correct = 0 \n",
    "        for file in files: \n",
    "            total += 1 \n",
    "            result = self.classify(directory + '/' + file)\n",
    "            if result == category: \n",
    "                correct += 1 \n",
    "        return (correct, total)\n",
    "    \n",
    "    def test(self, test_directory): \n",
    "        categories = os.listdir(test_directory)\n",
    "        correct = 0 \n",
    "        total = 0 \n",
    "        for category in categories: \n",
    "            (catCor, catTot) = self.testCat(test_directory + '/' + category, category)\n",
    "            correct += catCor \n",
    "            total += catTot \n",
    "        \n",
    "        print \"\\nACCURACY: %f , %i test instances\" % (float(correct)/ total, total)\n",
    "\n",
    "bT = BayesText(\"20news-bydate/20news-bydate-train\", \"20news-bydate/stoplist.txt\")    \n",
    "print \"\\nPROBABILITY OF WORD GOD IN MOTORCYCLES: \", bT.prob[\"rec.motorcycles\"][\"god\"]\n",
    "print \"PROBABILITY OF WORD GOD IN SOC RELIGION: \", bT.prob[\"soc.religion.christian\"][\"god\"]\n",
    "\n",
    "bT.classify(\"20news-bydate/20news-bydate-test/sci.med/59246\")\n",
    "bT.test('20news-bydate/20news-bydate-test') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes and Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to determine the <b>writer's opinion</b>! \n",
    "\n",
    "One common task is to find the <b>polarity</b> of a review. Naive Bayes can be used for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fold 0... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 1... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 2... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 3... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 4... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 5... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 6... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 7... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 8... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "Cross-fold 9... \n",
      "Perform count operation...\n",
      "->neg\n",
      "->pos\n",
      "Computing probabilities! :D\n",
      "->neg\n",
      "->pos\n",
      "Training completed.\n",
      "\n",
      "\n",
      "         neg         pos\n",
      "neg| 849.000000| 151.000000| \n",
      "\n",
      "pos| 212.000000| 788.000000| \n",
      "\n",
      "\n",
      "MODEL ACCURACY: 0.818500 PERCENT\n"
     ]
    }
   ],
   "source": [
    "import os, math, codecs \n",
    "\n",
    "class BayesText: \n",
    "    \n",
    "    def __init__(self, training_dir, stopword_list, igBucket): \n",
    "        self.vocab = {}\n",
    "        self.prob = {}\n",
    "        self.totals = {}\n",
    "        self.stopwords = {}\n",
    "\n",
    "        swFile = open(stopword_list) \n",
    "        for line in swFile.readlines():\n",
    "            self.stopwords[line.strip()] = 1 \n",
    "        swFile.close() \n",
    "        \n",
    "        #get list of categories from names of folders in train dir\n",
    "        categories = os.listdir(training_dir)\n",
    "        self.categories = [filename for filename in categories if os.path.isdir(training_dir + '/' + filename)]\n",
    "        print \"Perform count operation...\"\n",
    "        for category in self.categories: \n",
    "            print '->' + category \n",
    "            (self.prob[category], self.totals[category]) = self.train(training_dir, category, igBucket)\n",
    "        toBeDeleted = []\n",
    "        for word in self.vocab.keys():\n",
    "            if self.vocab[word] < 3: \n",
    "                toBeDeleted.append(word)\n",
    "        for word in toBeDeleted: \n",
    "            del self.vocab[word]\n",
    "        vocabLength = len(self.vocab)\n",
    "        print \"Computing probabilities! :D\"\n",
    "        for category in self.categories: \n",
    "            print '->' + category\n",
    "            denominator = self.totals[category] + vocabLength\n",
    "            for word in self.vocab: \n",
    "                if word in self.prob[category]:\n",
    "                    count = self.prob[category][word]\n",
    "                else: \n",
    "                    count = 1 \n",
    "                self.prob[category][word] = float(count + 1) / denominator\n",
    "            \n",
    "        print \"Training completed.\"\n",
    "    \n",
    "    def train(self, training_dir, category, igBucket): \n",
    "        ignore = \"%i\" % igBucket\n",
    "        current_dir = training_dir + '/' + category \n",
    "        directories = os.listdir(current_dir)\n",
    "        counts = {}\n",
    "        total = 0\n",
    "        for directory in directories: \n",
    "            if directory != ignore: \n",
    "                currentBucket = training_dir + '/' + category + '/' + directory\n",
    "                files = os.listdir(currentBucket)\n",
    "                for file in files: \n",
    "                    f = codecs.open(currentBucket + '/' + file, 'r', 'iso8859-1')\n",
    "                    for line in f: \n",
    "                        tokens = line.split()\n",
    "                        for token in tokens: \n",
    "                            token = token.strip('\\'\".,?:-')\n",
    "                            token = token.lower() \n",
    "                            if token != '' and not token in self.stopwords: \n",
    "                                self.vocab.setdefault(token, 0)\n",
    "                                self.vocab[token] += 1 \n",
    "                                counts.setdefault(token, 0)\n",
    "                                counts[token] += 1 \n",
    "                                total += 1 \n",
    "                    f.close() \n",
    "        return (counts,total)\n",
    "    \n",
    "    def classify(self, train_file): \n",
    "        results = {}\n",
    "        for category in self.categories: \n",
    "            results[category] = 0 \n",
    "        f = codecs.open(train_file, 'r', 'iso8859-1')\n",
    "        for line in f: \n",
    "            tokens = line.split() \n",
    "            for token in tokens: \n",
    "                token = token.strip('\\'\".,?:-').lower() \n",
    "                if token in self.vocab: \n",
    "                    for category in self.categories: \n",
    "                        if self.prob[category][token] == 0: \n",
    "                            print \"%s %s\" % (category, token)\n",
    "                        results[category] += math.log(self.prob[category][token])\n",
    "        f.close() \n",
    "        results = list(results.items())\n",
    "        results.sort(key= lambda t: t[1], reverse = True)\n",
    "        \n",
    "        return results[0][0]\n",
    "                        \n",
    "    def testCat(self, di, category, bucket): \n",
    "        r = {}\n",
    "        directory = di + (\"/%i/\" % bucket) \n",
    "        files = os.listdir(directory)\n",
    "        total = 0 \n",
    "        correct = 0 \n",
    "        for file in files: \n",
    "            total += 1 \n",
    "            res = self.classify(directory + '/' + file)\n",
    "            r.setdefault(res, 0)\n",
    "            r[res] += 1 \n",
    "        return r \n",
    "        \n",
    "    def test(self, test_directory, bucket): \n",
    "        r = {}\n",
    "        categories = os.listdir(test_directory)\n",
    "        correct = 0 \n",
    "        total = 0         \n",
    "        for category in categories: \n",
    "            r[category] = self.testCat(test_directory + '/' + category, category, bucket)\n",
    "        return r \n",
    "            \n",
    "def tenfold(pref, stop): \n",
    "    results = {}\n",
    "    for i in range(10): \n",
    "        print \"Cross-fold %i... \" % i\n",
    "        bT = BayesText(pref, stop, i)\n",
    "        r = bT.test(\"review_polarity_buckets/txt_sentoken\", i)\n",
    "        for (k, v) in r.iteritems(): \n",
    "            results.setdefault(k,{})\n",
    "            for (ck, cv) in v.iteritems(): \n",
    "                results[k].setdefault(ck,0)\n",
    "                results[k][ck] += cv \n",
    "                categories = list(results.keys())\n",
    "    categories.sort() \n",
    "    tot = 0\n",
    "    correct = 0 \n",
    "    print \"\\n\\n         neg         pos\"\n",
    "    for (true_class, prediction_vals) in results.iteritems(): \n",
    "        print \"%s|\" % true_class, \n",
    "        for (predicted_class, total) in prediction_vals.iteritems(): \n",
    "            if predicted_class == true_class: \n",
    "                correct += total \n",
    "            tot += total \n",
    "            print \"%f|\" % total, \n",
    "        print \"\\n\"\n",
    "    \n",
    "    print \"\\nMODEL ACCURACY: %f PERCENT\" % (float(correct) / tot) \n",
    "        \n",
    "tenfold(\"review_polarity_buckets/txt_sentoken\",\"review_polarity_buckets/stopwords174.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
