{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluating algorithms and kNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<ul> \n",
    "    <li>\n",
    "        <b>Training:</b> used to train classifier, and create the model \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Test:</b> used to evaulate the model \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<b>NEVER</b> test model using training data! Often yields overly optimistic results. Might make sense to split data into independent sets (as seen before), train on one, and test with another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 10-Fold Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Common data science technique where data is split into <b>10 distinct groups</b>. At each phase, <b>9 of the 10 partitions</b> are used to train the model while <b>the other partition is used as our test set</b> and then <b> average the results</b> \n",
    "\n",
    "This method can yield <b>different results</b> based on the partitions, which can be suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### n-Fold Cross Validation (Leave-one-out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Same approach as last time, except each iteration we leave <b>one datapoint</b> out of our training set, and then test on this datapoint. Benefit of this approach is that it's <b>deterministic</b>, meaning you will get the same results every time. \n",
    "\n",
    "Drabacks of leave-one-out: \n",
    "<ul> \n",
    "    <li><b>Huge</b> computational cost to this method.</li> \n",
    "    <li><b>Stratification:</b> sample in each <i>bucket</i> should be representative of <b>entire population!</b> LOO does not satisfy this stratification, since it necessarily leaves only one data element out.</li> \n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix with \"actual class\" as the rows and \"predicted class\" as the columns. This means the <b>diagnols represent correct predictions</b> while the other values are incorrect. \n",
    "\n",
    "Straighforward to look at confusion matrix and get a glimpse at <b>where your classifier is failing</b>. \n",
    "\n",
    "Lets grab the classifier code from last chapter, modify it to perform 10-fold cross validation, and show a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classified as: \n",
      "   10   15   20   25   30   35   40   45 \n",
      " +----+----+----+----+----+----+----+----+\n",
      "10 | 0 |13 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "15 | 0 |62 |23 | 1 | 0 | 0 | 0 | 0 |\n",
      "20 | 0 |11 |62 |17 | 1 | 5 | 0 | 0 |\n",
      "25 | 0 | 1 |13 |35 |17 |13 | 0 | 0 |\n",
      "30 | 0 | 0 | 6 |21 |23 | 7 | 6 | 0 |\n",
      "35 | 0 | 0 | 2 |12 |19 | 2 | 3 | 0 |\n",
      "40 | 0 | 0 | 1 | 2 | 7 | 0 | 1 | 0 |\n",
      "45 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 0 |\n",
      " +----+----+----+----+----+----+----+----+\n",
      "\n",
      "47.194 percent correct!!!\n",
      "Total of 392 instances.\n",
      "\n",
      " Classified as: \n",
      "   10   15   20   25   30   35   40   45 \n",
      " +----+----+----+----+----+----+----+----+\n",
      "10 | 0 |13 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "15 | 0 |63 |22 | 1 | 0 | 0 | 0 | 0 |\n",
      "20 | 0 |12 |60 |24 | 0 | 0 | 0 | 0 |\n",
      "25 | 0 | 0 |15 |53 |11 | 0 | 0 | 0 |\n",
      "30 | 0 | 0 | 6 |32 |25 | 0 | 0 | 0 |\n",
      "35 | 0 | 0 | 2 |17 |19 | 0 | 0 | 0 |\n",
      "40 | 0 | 0 | 1 | 3 | 7 | 0 | 0 | 0 |\n",
      "45 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 0 |\n",
      " +----+----+----+----+----+----+----+----+\n",
      "\n",
      "51.276 percent correct!!!\n",
      "Total of 392 instances.\n"
     ]
    }
   ],
   "source": [
    "class Classifier:\n",
    "\n",
    "    #add \"K\" parameter\n",
    "    def __init__(self, bucketPrefix, testBucketNumber, dataFormat, k=1):\n",
    "        self.k = k \n",
    "\n",
    "        #same stuff as always\n",
    "        self.medianAndDeviation = []\n",
    "        self.data = [] \n",
    "        \n",
    "        #load and format datatypes per column \n",
    "        self.format = dataFormat.strip().split(' ')\n",
    "        \n",
    "        ##load in each training set that ISNT OUR TEST BUCKET! \n",
    "        for i in range(1,11): \n",
    "            if i != testBucketNumber: \n",
    "                filename = \"%s-%02i\" % (bucketPrefix, i)\n",
    "                # reading the data in from the file\n",
    "                f = open(filename)\n",
    "                lines = f.readlines()\n",
    "                f.close()\n",
    "                self.data = []\n",
    "                for line in lines[1:]:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    ignore = []\n",
    "                    vector = []\n",
    "                    for i in range(len(fields)):\n",
    "                        if self.format[i] == 'num':\n",
    "                            vector.append(float(fields[i]))\n",
    "                        elif self.format[i] == 'comment':\n",
    "                            ignore.append(fields[i])\n",
    "                        elif self.format[i] == 'class':\n",
    "                            classification = fields[i]\n",
    "                    self.data.append((classification, vector, ignore))\n",
    "        self.rawData = list(self.data)\n",
    "        # get length of instance vector\n",
    "        self.vlen = len(self.data[0][1])\n",
    "        # now normalize the data\n",
    "        for i in range(self.vlen):\n",
    "            self.normalizeColumn(i)\n",
    "        \n",
    "    \n",
    "    #takes test bucket information and runs it based on curre\n",
    "    def testBucket(self, bucketPrefix, bucketNumber):\n",
    "        #load test file \n",
    "        filename = \"%s-%02i\" % (bucketPrefix, bucketNumber)\n",
    "        f = open(filename)\n",
    "        lines = f.readlines()\n",
    "        totals = {}\n",
    "        f.close()\n",
    "        #load data for each line \n",
    "        for line in lines:\n",
    "            data = line.strip().split('\\t')\n",
    "            vector = []\n",
    "            classInColumn = -1\n",
    "            for i in range(len(self.format)):\n",
    "                if self.format[i] == 'num':\n",
    "                    vector.append(float(data[i]))\n",
    "                elif self.format[i] == 'class':\n",
    "                    classInColumn = i\n",
    "            #make predictions based on training data\n",
    "            theRealClass = data[classInColumn]\n",
    "            classifiedAs = self.classify(vector)\n",
    "            totals.setdefault(theRealClass, {})\n",
    "            #tally up totals \n",
    "            totals[theRealClass].setdefault(classifiedAs, 0)\n",
    "            totals[theRealClass][classifiedAs] += 1\n",
    "        return totals\n",
    "\n",
    "    def getMedian(self, alist):\n",
    "        \"\"\"return median of alist\"\"\"\n",
    "        if alist == []:\n",
    "            return []\n",
    "        blist = sorted(alist)\n",
    "        length = len(alist)\n",
    "        if length % 2 == 1:\n",
    "            # length of list is odd so return middle element\n",
    "            return blist[int(((length + 1) / 2) -  1)]\n",
    "        else:\n",
    "            # length of list is even so compute midpoint\n",
    "            v1 = blist[int(length / 2)]\n",
    "            v2 =blist[(int(length / 2) - 1)]\n",
    "            return (v1 + v2) / 2.0\n",
    "        \n",
    "\n",
    "    def getAbsoluteStandardDeviation(self, alist, median):\n",
    "        \"\"\"given alist and median return absolute standard deviation\"\"\"\n",
    "        sum = 0\n",
    "        for item in alist:\n",
    "            sum += abs(item - median)\n",
    "        return sum / len(alist)\n",
    "\n",
    "\n",
    "    def normalizeColumn(self, columnNumber):\n",
    "        col = [v[1][columnNumber] for v in self.data]\n",
    "        median = self.getMedian(col)\n",
    "        asd = self.getAbsoluteStandardDeviation(col, median)\n",
    "        self.medianAndDeviation.append((median, asd))\n",
    "        for v in self.data:\n",
    "            v[1][columnNumber] = (v[1][columnNumber] - median) / asd\n",
    "\n",
    "\n",
    "    def normalizeVector(self, v):\n",
    "        \"\"\"We have stored the median and asd for each column.\n",
    "        We now use them to normalize vector v\"\"\"\n",
    "        vector = list(v)\n",
    "        for i in range(len(vector)):\n",
    "            (median, asd) = self.medianAndDeviation[i]\n",
    "            vector[i] = (vector[i] - median) / asd\n",
    "        return vector\n",
    "\n",
    "    \n",
    "    ###\n",
    "    ### END NORMALIZATION\n",
    "    ##################################################\n",
    "\n",
    "\n",
    "\n",
    "    def manhattan(self, vector1, vector2):\n",
    "        \"\"\"Computes the Manhattan distance.\"\"\"\n",
    "        return sum(map(lambda v1, v2: abs(v1 - v2), vector1, vector2))\n",
    "\n",
    "    def euclidean(self, vector1, vector2): \n",
    "        return sum(map(lambda v1, v2: (v1 - v2) ** 2, vector1, vector2)) ** (0.5)\n",
    "\n",
    "    def kNN(self, itemVector):\n",
    "        neighbors = sorted([(self.manhattan(itemVector, item[1]), item)\n",
    "                     for item in self.data])[:self.k]\n",
    "        votes = {} \n",
    "        for neighbor in neighbors: \n",
    "            votes.setdefault(neighbor[1][0], 0)\n",
    "            votes[neighbor[1][0]] += 1 \n",
    "\n",
    "        sortedVotes = sorted(votes, votes.get, reverse=False)\n",
    "        prediction = sortedVotes[0]\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def classify(self, itemVector):\n",
    "        \"\"\"Return class we think item Vector is in\"\"\"\n",
    "        return(self.kNN(self.normalizeVector(itemVector)))\n",
    "\n",
    "#INTERESTING! create new classifier for each validation iteration \n",
    "def tenfoldCrossValidation(bucketPrefix, dataFormat,kN=1): \n",
    "    results = {}\n",
    "    for i in range(1,11): \n",
    "        #create and train a new classifier for each combination of 9 buckets\n",
    "        c = Classifier(bucketPrefix, i, dataFormat,k=kN) \n",
    "        test = c.testBucket(bucketPrefix, i)\n",
    "        for (key, value) in test.items(): \n",
    "            results.setdefault(key, {})\n",
    "            for (categoryKey, categoryValue) in value.items(): \n",
    "                results[key].setdefault(categoryKey, 0) \n",
    "                results[key][categoryKey] += categoryValue \n",
    "    categories = list(results.keys())\n",
    "    categories.sort() \n",
    "    \n",
    "    print( \"\\n Classified as: \")\n",
    "    header = \" \"\n",
    "    subheader = \" +\"\n",
    "    for category in categories:\n",
    "        header += \"  \" + category + \" \"\n",
    "        subheader += \"----+\"\n",
    "    print (header)\n",
    "    print (subheader)\n",
    "    total = 0.0\n",
    "    correct = 0.0    \n",
    "    for category in categories:\n",
    "        row = category + \" |\"\n",
    "        for c2 in categories:\n",
    "            if c2 in results[category]:\n",
    "                count = results[category][c2]\n",
    "            else:\n",
    "                count = 0\n",
    "            row += \"%2i |\" % count\n",
    "            total += count\n",
    "            if c2 == category:\n",
    "                correct += count\n",
    "        print(row)\n",
    "    print(subheader)\n",
    "    print(\"\\n%5.3f percent correct!!!\" %((correct * 100) / total))\n",
    "    print(\"Total of %i instances.\" % total)\n",
    "    \n",
    "tenfoldCrossValidation(\"mpgData/mpgData\", \"class num num num num num comment\",1) #1 nearest neighbor\n",
    "tenfoldCrossValidation(\"mpgData/mpgData\", \"class num num num num num comment\",2) #2 nearest neighbors\n",
    "\n",
    "#HOLY CRAP IT WORKED THAT IS AWESOME!\n",
    "#for some reason, the classifier I wrote is mysteriously 5% less accurate than the one he wrote? hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kappa statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares performance of classifier to classifier that makes decisions based solely on chance. \n",
    "\n",
    "<ul> \n",
    "    <li>Take <b>row totals</b>, which represent amount of true datapoints within that class </li> \n",
    "    <li>Take <b>column totals</b> which represent total predicted values for classifier </li> \n",
    "    <li>Predict (Column Total / Total Values) * Amount for Class classifications in our confusion matrix for each class</li> \n",
    "</ul> \n",
    "\n",
    "$$ k = \\frac{P(c) - P(r)}{1 - P(r)} $$\n",
    "\n",
    "Kappa statistic is given by the above formula, where P(c) is accuracy of the real classifier and P(r) is the accuracy of our (semi?) random classifier. How do we interperet the value given by this heuristic? \n",
    "\n",
    "<ul> \n",
    "    <li>\n",
    "        <b>Less than 0:</b> less than random chance!!!! an octupus pointing at a piece of paper could have done better than your code did.\n",
    "    </li>\n",
    "    \n",
    "    <li>\n",
    "        <b>0.01-0.20:</b> slightly good \n",
    "    </li>\n",
    "    \n",
    "    <li>\n",
    "        <b>0.21-0.4:</b> fair performance\n",
    "    </li>\n",
    "    \n",
    "    <li>\n",
    "        <b>0.41-0.6:</b> moderate performance\n",
    "    </li>\n",
    "    \n",
    "    <li>\n",
    "        <b>0.61-0.8:</b> substantially good performance\n",
    "    </li>\n",
    "    \n",
    "    <li>\n",
    "        <b>0.81-1.00:</b> near perfect performance\n",
    "    </li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements to the Nearest Neighbor Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Rote classifier:</b> not useful in practice, but it only classifies EXACT MATCHES to datapoints it is trained on</li>\n",
    "    <li><b>Nearest neighbor</b> classifier can be seen as an extension of this... the <i><b>IF IT WALKS LIKE A DUCK, AND IT QUACKS LIKE A DUCK, IT PROBABLY IS A DUCK</b></i> approach</li>\n",
    "    <li>Nearest neighbor approach has the same problem as it did with recommendations... <b>sensitive to outliers</b>. k-Nearest Neighbors mitigates this sensitivity to outliers.</li>\n",
    "</ul> \n",
    "\n",
    "When predicting a <b>discrete class</b>, the k-nearest neighbors <b>cast votes</b> for which class the item in question should be in, and randomly selects if there is a tie. In cases with <b>numeric values</b>, as we saw in recommender systems earlier, we can weight averages based on user similarity. \n",
    "\n",
    "For given distances, we should 1) replace distance with <b>inverse distance</b> (smaller become larger) and 2) <b>divide each inverse distance with the sum of all inverse distances</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classified as: \n",
      "   0   1 \n",
      " +----+----+\n",
      "0 |41 |18 |\n",
      "1 |14 |27 |\n",
      " +----+----+\n",
      "\n",
      "68.000 percent correct!!!\n",
      "Total of 100 instances.\n",
      "\n",
      " Classified as: \n",
      "   0   1 \n",
      " +----+----+\n",
      "0 |50 | 9 |\n",
      "1 |23 |18 |\n",
      " +----+----+\n",
      "\n",
      "68.000 percent correct!!!\n",
      "Total of 100 instances.\n"
     ]
    }
   ],
   "source": [
    "tenfoldCrossValidation(\"pimaSmall/pimaSmall\", \"num num num num num num num num class\",3) #3 nearest neighbors\n",
    "tenfoldCrossValidation(\"pimaSmall/pimaSmall\", \"num num num num num num num num class\",1) #1 nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE DATA IS MORE IMPORTANT THAN A BETTER ALGORITHM!!!!!!!!!!!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
