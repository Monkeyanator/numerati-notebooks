{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections, our datapoints have been labeled. What about when we <b>don't</b> have these labels? What happens if I want to discover <b>possible groups?</b>\n",
    "\n",
    "<b>k-means clustering</b> is clustering where we tell the algorithm <i>how many clusters to make</i>. For example: <i>\"cluster these people into 5 groups\"</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we don't tell the algorithm how many clusters to make. \n",
    "\n",
    "This algorithm starts <b>with each instance in its own cluster</b>. Each iteration of the algorithm <b>combines the two most similar clusters into one</b>. Repeats until there is only <b>one cluster</b>.\n",
    "\n",
    "This is called <b>hierarchial clustering</b>. The algorithm results in <b>one huge cluster</b> with one <b>sub-cluster</b>. Each subcluster has its own subcluster, etc., etc.\n",
    "\n",
    "Remember, <b><u>AT EACH ITERATION, WE JOIN THE TWO NEAREST CLUSTERS!</u></b>\n",
    "\n",
    "### Single-linkage clustering \n",
    "\n",
    "In this method, we define the distance between clusters as the <b>shortest distance between any member of one cluster to any member of the other</b>. \n",
    "\n",
    "### Complete-linkage clustering\n",
    "\n",
    "In this method, we define the distance between two clusters as <b>the greatest distance between any member of one cluster to any member of the other</b>. \n",
    "\n",
    "### Average-linkage clustering\n",
    "\n",
    "In average-linkage clustering we define the distance between two clusters as <b>the average distance between any member of one cluster to any member of another</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding hierarchial clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 'Enrico Vittadini')\n",
      "(15, 'Kati Piri')\n",
      "(20, 'Perseporino')\n"
     ]
    }
   ],
   "source": [
    "#example of how priority queues work\n",
    "from Queue import PriorityQueue\n",
    "\n",
    "#queue that spits out elements based not on order in our out \n",
    "#but rather, some parameter attached with the data\n",
    "sQueue = PriorityQueue() \n",
    "sQueue.put((15, \"Kati Piri\"))\n",
    "sQueue.put((20, \"Perseporino\"))\n",
    "sQueue.put((10, \"Enrico Vittadini\"))\n",
    "\n",
    "#retrieves elements with the first element of the tuple as priority\n",
    "s1 = sQueue.get() \n",
    "s2 = sQueue.get() \n",
    "s3 = sQueue.get() \n",
    "\n",
    "print s1 \n",
    "print s2 \n",
    "print s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTERED GROUPINGS\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "(('Chihuahua', 'Yorkshire Terrier'), ('Great Dane', ('Bullmastiff', (('German Shepherd', 'Golden Retriever'), ('Standard Poodle', ('Boston Terrier', ('Brittany Spaniel', ('Border Collie', 'Portuguese Water Dog'))))))))\n"
     ]
    }
   ],
   "source": [
    "#below is an implementation of hierarchial clustering \n",
    "import math \n",
    "from Queue import PriorityQueue\n",
    "\n",
    "#UTILITY FUNCS\n",
    "def median(l): \n",
    "    temp = list(l)\n",
    "    temp.sort()\n",
    "    length = len(temp)\n",
    "    if (length % 2 == 1): \n",
    "        return temp[int(length / 2)]\n",
    "    else: \n",
    "        return (temp[int(length / 2)] + temp[int(length / 2) - 1]) / 2 \n",
    "\n",
    "#all the columns are bound to be on different scales- \n",
    "#normalize all that jazz! \n",
    "def normalizeCol(col): \n",
    "    med = median(col) \n",
    "    adjSd = sum([abs(x - med) for x in col]) / len(col)\n",
    "    return [(x - med) / adjSd for x in col]\n",
    "\n",
    "class HierarchialClusterer: \n",
    "    \n",
    "    #accept a datafile to initialize the clusterer\n",
    "    def __init__(self, dataFile): \n",
    "        f = open(dataFile)\n",
    "        self.data = {}\n",
    "        self.counter = 0 \n",
    "        self.queue = PriorityQueue()\n",
    "        lines = f.readlines() \n",
    "        f.close() \n",
    "        \n",
    "        header = lines[0].split(',')\n",
    "        self.cols = len(header)\n",
    "        self.data = [[] for i in range(self.cols)]\n",
    "        \n",
    "        #exclude the first line w/ list splice magic! \n",
    "        for line in lines[1:]:\n",
    "            cells = line.split(',')\n",
    "            t = 0 \n",
    "            for cell in range(self.cols): \n",
    "                if t == 0: \n",
    "                    self.data[cell].append(cells[cell])\n",
    "                    t = 1 \n",
    "                else: \n",
    "                    self.data[cell].append(float(cells[cell]))\n",
    "        \n",
    "        #GOTTA NORMALIZE THE DATA WITH THAT nifty FUNCTION WE WROTE!\n",
    "        for i in range(1, self.cols): \n",
    "            self.data[i] = normalizeCol(self.data[i])\n",
    "        \n",
    "        #Here we go. The described algorithm. This shall be brutal. \n",
    "        #The major steps: \n",
    "        #1) Calculate Euclidean Distance from i to each other element\n",
    "        #store the result in 'neighbors', a dict.\n",
    "        #2) Find nearest neighbor \n",
    "        #3) Place on queue\n",
    "        \n",
    "        rows = len(self.data[0])\n",
    "        \n",
    "        for i in range(rows): \n",
    "            minDist = 99999\n",
    "            nn = 0 \n",
    "            neighbors = {}\n",
    "            for j in range(rows): \n",
    "                if i != j: \n",
    "                    d = self.distance(i,j)\n",
    "                    if i < j: \n",
    "                        pair = (i,j)\n",
    "                    else: \n",
    "                        pair = (j,i)\n",
    "                        \n",
    "                    #set j'th element of neighbors to dist and pair\n",
    "                    neighbors[j] = (pair, d)\n",
    "                    \n",
    "                    if d < minDist: \n",
    "                        minDist = d \n",
    "                        nn = j \n",
    "                        nearestNum = j \n",
    "                    \n",
    "            if i < nn: \n",
    "                nearestPair = (i, nn)\n",
    "            else: \n",
    "                nearestPair = (nn, i)\n",
    "            \n",
    "            self.queue.put((minDist, self.counter, [[self.data[0][i]], nearestPair, neighbors]))\n",
    "            self.counter += 1 \n",
    "    \n",
    "    def distance(self, i, j): \n",
    "        ss = 0 \n",
    "        for k in range(1, self.cols): \n",
    "            ss += (self.data[k][i] - self.data[k][j]) ** 2 \n",
    "        return math.sqrt(ss)\n",
    "    \n",
    "    def cluster(self): \n",
    "        finished = False \n",
    "        while not finished: \n",
    "            top1 = self.queue.get() \n",
    "            nearestPair = top1[2][1]\n",
    "            if not self.queue.empty(): \n",
    "                next1 = self.queue.get() \n",
    "                nearPair = next1[2][1]\n",
    "                tmp = []\n",
    "                \n",
    "                #so obviously, if the closest distance is from i to j,\n",
    "                #the next closest should be the same, from j to i \n",
    "                #although there could be overlaps with other pairs \n",
    "                #so go ahead and pop off until we hit a duplicate \n",
    "                \n",
    "                while nearPair != nearestPair: \n",
    "                    tmp.append((next1[0], self.counter, next1[2]))\n",
    "                    self.counter += 1 \n",
    "                    next1 = self.queue.get() \n",
    "                    nearPair = next1[2][1]\n",
    "                \n",
    "                #clean up the mess we made :D \n",
    "                for item in tmp:\n",
    "                    self.queue.put(item)\n",
    "                \n",
    "                if len(top1[2][0]) == 1: \n",
    "                    item1 = top1[2][0][0]\n",
    "                else:\n",
    "                    item1 = top1[2][0]\n",
    "                    \n",
    "                if len(next1[2][0]) == 1: \n",
    "                    item2 = next1[2][0][0]\n",
    "                else: \n",
    "                    item2 = next1[2][0]\n",
    "                \n",
    "                currentCluster = (item1, item2)\n",
    "                \n",
    "                #now find NN for this cluster, and build new neighbors list \n",
    "                \n",
    "                minDist = 99999\n",
    "                nearestPair = ()\n",
    "                nearestNeighbor = ''\n",
    "                merged = {}\n",
    "                nNeighbors = next1[2][2]\n",
    "                for(key, value) in top1[2][2].items(): \n",
    "                    if key in nNeighbors: \n",
    "                        if nNeighbors[key][1] < value[1]: \n",
    "                            dist = nNeighbors[key]\n",
    "                        else: \n",
    "                            dist = value \n",
    "                        if dist[1] < minDist: \n",
    "                            minDist = dist[1]\n",
    "                            nearestPair = dist[0]\n",
    "                            nearestNeighbor = key \n",
    "                        #set merged element to current dist \n",
    "                        merged[key] = dist \n",
    "                \n",
    "                #if empty\n",
    "                if merged == {}:\n",
    "                    return currentCluster \n",
    "                else: \n",
    "                    self.queue.put((minDist, self.counter, [currentCluster, nearestPair, merged]))\n",
    "                    self.counter += 1 \n",
    "\n",
    "fName = 'dogs.csv'\n",
    "clusterer = HierarchialClusterer(fName)\n",
    "cluster = clusterer.cluster() \n",
    "\n",
    "print 'CLUSTERED GROUPINGS\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-'\n",
    "print cluster \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Means is <b>the most popular</b> clustering algorithm. \n",
    "\n",
    "Logical and intuitive process, the steps are as such: \n",
    "<ol> \n",
    "    <li><b>Step 1:</b> select k random instances to be the initial centroid</li> \n",
    "    <li><b>Step 2:</b> REPEAT</li> \n",
    "    <li><b>Step 3:</b> assign each instance to the nearest centroid, which preserves the amount of clusters</li> \n",
    "    <li><b>Step 4:</b> update centroids by computing mean of each cluster</li> \n",
    "    <li><b>Step 5:</b> UNTIL centroids don't change (significantly)</li> \n",
    "</ol>\n",
    "\n",
    "Algorithm is said to <b>converge</b> when the points in each group stop shifting around. \n",
    "\n",
    "Possible to relax our criteria of <i>stop when no points shift</i> to <i>stop when less than 1% of points shift</i>, and maintain solid results. \n",
    "\n",
    "<b>K-MEANS is an instance of Expectation-Maximization (EM)</b>, which is a method that has two phases.\n",
    "<ul>\n",
    "    <li><b>E:</b> use estimate to place points into their expected cluster</li> \n",
    "    <li><b>M:</b> use these expected values to adjust the estimate of the centroids</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE or Scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the <b>sum of the squared error</b> to determine the quality of a set of clusters (we also call this <b>scatter</b>) \n",
    "\n",
    "For each point, square the distance from that point to its centroid, then add those squared distances together. \n",
    "\n",
    "$$ \\sum_{i=1}^{k}\\sum_{x\\in{C_i}}{dist(c_i,x)^2} $$ \n",
    "\n",
    "All this means is \"iterate over the clusters, and sum up the squared differences from each point to the current cluster\". \n",
    "\n",
    "This metric lets us know how well our data is clustered. Smaller SSE means the clusters are better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Class 1\n",
      "-=-=-=-=-\n",
      "Chihuahua\n",
      "Yorkshire Terrier\n",
      "\n",
      "\n",
      "Class 2\n",
      "-=-=-=-=-\n",
      "Bullmastiff\n",
      "German Shepherd\n",
      "Great Dane\n",
      "\n",
      "\n",
      "Class 3\n",
      "-=-=-=-=-\n",
      "Border Collie\n",
      "Boston Terrier\n",
      "Brittany Spaniel\n",
      "Golden Retriever\n",
      "Portuguese Water Dog\n",
      "Standard Poodle\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "#UTILITY FUNCS, SAME AS BEFORE\n",
    "\n",
    "def median(l): \n",
    "    temp = list(l)\n",
    "    temp.sort()\n",
    "    length = len(temp)\n",
    "    if (length % 2 == 1): \n",
    "        return temp[int(length / 2)]\n",
    "    else: \n",
    "        return (temp[int(length / 2)] + temp[int(length / 2) - 1]) / 2 \n",
    "\n",
    "#all the columns are bound to be on different scales- \n",
    "#normalize all that jazz! \n",
    "def normalizeCol(col): \n",
    "    med = median(col) \n",
    "    adjSd = sum([abs(x - med) for x in col]) / len(col)\n",
    "    return [(x - med) / adjSd for x in col]\n",
    "\n",
    "class kMeansClusterer: \n",
    "    #k-means clustering class \n",
    "    \n",
    "    def __init__(self, dataFile, k): \n",
    "        f = open(dataFile)\n",
    "        \n",
    "        #init members\n",
    "        self.data = {}\n",
    "        self.k = k \n",
    "        self.counter = 0  \n",
    "        self.iterationNumber = 0\n",
    "        self.pointsChanged = 0 \n",
    "        self.sse = 0 \n",
    "        \n",
    "        lines = f.readlines() \n",
    "        f.close() \n",
    "        \n",
    "        header = lines[0].split(',') \n",
    "        self.cols = len(header)\n",
    "        self.data = [[] for i in range(self.cols)]\n",
    "        \n",
    "        #read file into data dict\n",
    "        for line in lines[1:]: \n",
    "            cells = line.split(',')\n",
    "            t = 0\n",
    "            for cell in range(self.cols): \n",
    "                if t == 0: \n",
    "                    self.data[cell].append(cells[cell])\n",
    "                    t = 1 \n",
    "                else: \n",
    "                    self.data[cell].append(float(cells[cell]))\n",
    "        \n",
    "        self.dSize = len(self.data[1])\n",
    "        self.memberOf = [-1 for x in range(len(self.data[1]))] \n",
    "        \n",
    "        #normalize \n",
    "        for i in range(1,self.cols): \n",
    "            self.data[i] = normalizeCol(self.data[i])\n",
    "            \n",
    "        #seed randomizer and pick initial centroid \n",
    "        random.seed()\n",
    "        self.centroids = [[self.data[i][r] for i in range(1,len(self.data))] for r in random.sample(range(len(self.data[0])),self.k)]\n",
    "        self.assignPointsToCluster() \n",
    "        \n",
    "    def assignPointToCluster(self, i): \n",
    "        minimum = 999999\n",
    "        clusterNumber = -1 \n",
    "        #iterate over the clusters finding distance \n",
    "        for centroid in range(self.k): \n",
    "            dist = self.euclideanDistance(i, centroid)\n",
    "            if dist < minimum: \n",
    "                minimum = dist\n",
    "                clusterNumber = centroid \n",
    "        #changed points\n",
    "        if clusterNumber != self.memberOf[i]:\n",
    "            self.pointsChanged += 1 \n",
    "        \n",
    "        self.sse = self.sse + minimum ** 2 \n",
    "        return clusterNumber\n",
    "\n",
    "    def updateCentroids(self): \n",
    "        members = [self.memberOf.count(i) for i in range(len(self.centroids))]\n",
    "        self.centroids = [[sum([self.data[k][i] for i in range(len(self.data[0])) if self.memberOf[i] == centroid])/members[centroid] for k in range(1,len(self.data))] for centroid in range(len(self.centroids))]\n",
    "\n",
    "    def assignPointsToCluster(self): \n",
    "        self.pointsChanged =0\n",
    "        self.sse =0\n",
    "        self.memberOf = [self.assignPointToCluster(i) for i in range(len(self.data[1]))]\n",
    "    \n",
    "    def euclideanDistance(self, i, j): \n",
    "        #comp dist from point i to the centroid j \n",
    "        ss = 0 \n",
    "        for k in range(1, self.cols): \n",
    "            ss += (self.data[k][i] - self.centroids[j][k-1]) ** 2 \n",
    "        return math.sqrt(ss)\n",
    "    \n",
    "    #actually cluster \n",
    "    def cluster(self):\n",
    "        done = False \n",
    "        while not done: \n",
    "            self.iterationNumber += 1 \n",
    "            self.updateCentroids() \n",
    "            self.assignPointsToCluster() \n",
    "            if float(self.pointsChanged) / len(self.memberOf) < 0.01: \n",
    "                done = True \n",
    "\n",
    "    def present(self): \n",
    "        for centroid in range(len(self.centroids)): \n",
    "            print \"\\n\\nClass %i\\n-=-=-=-=-\" % (centroid + 1)\n",
    "            for name in [self.data[0][i] for i in range(len(self.data[0])) if self.memberOf[i] == centroid]: \n",
    "                print name \n",
    "                \n",
    "klustahs = kMeansClusterer('dogs.csv', 3)\n",
    "klustahs.cluster() \n",
    "klustahs.present() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means++ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major weakness of the original k-means algorithm is that it <b>randomly</b> picks <i>k</i> datapoints to be the initial centroids.\n",
    "\n",
    "This randomness means that sometimes the initial centroids are great picks and lead to optimal clusters, and sometimes not. \n",
    "\n",
    "This method <b>changes the way we choose our initial clusters</b>, and it works as follows: \n",
    "<ol>\n",
    "    <li>Empty set of initial centroids</li> \n",
    "    <li>Select first centroid <b>randomly</b> from datapoints as before</li>\n",
    "    <li>Repeat until we have k initial centroids\n",
    "        <ul>\n",
    "            <li><b>a) </b>Compute distance between each datapoint and nearest centroid.</li>\n",
    "            <li><b>b) </b>In a probability with proportional to the distance, select one datapoint at random to be a new centroid and add it to set of centroids</li>\n",
    "            <li><b>c) </b>Repeat!</li>\n",
    "        </ul> \n",
    "\n",
    "    </li>\n",
    "</ol> \n",
    "\n",
    "The main idea is this: <b>whlie we still pick the initial centroids randomly, we prefer centroids that are far away from one another</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is about <b>discovery!</b>\n",
    "\n",
    "Examples: \n",
    "<ul> \n",
    "    <li>Useful to cluster search results!</li>\n",
    "    <li>Marketing teams might cluster users into demographics and target ads to each cluster</li> \n",
    "</ul> \n",
    "\n",
    "<b>When to use k-means over hierarchial clustering?</b> \n",
    "<ul>\n",
    "    <li><b>k-means:</b> simple and fast algorithm. Perfect first-step method to identify features of data.</li> \n",
    "    <li><b>hierarchial:</b> when we want to create a taxonomy or hierarchy in our data. not as memory efficient as our other method.</li> \n",
    "</ul> \n",
    "    \n",
    "<b>The ENRON DATASET HAS SOME COOL PATTERNS!</b> Over 600,000 emails were leaked after the Enron scandal, and they comprise a famous dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
